{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "5C-718ykIafI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "\n",
        "Answer:\n",
        "1. What is Logistic Regression?\n",
        "\n",
        "Logistic Regression is a supervised machine learning algorithm used for classification problems, especially binary classification (e.g., yes/no, default/no default).\n",
        "\n",
        "It predicts the probability that a given input belongs to a particular class.\n",
        "\n",
        "Uses the logistic (sigmoid) function to map any real-valued number to a value between 0 and 1:\n",
        "\n",
        "\n",
        "The predicted probability can then be converted to a class label using a threshold (commonly 0.5).\n",
        "\n",
        "\n",
        "2. Key Idea\n",
        "\n",
        "Instead of predicting a continuous output (like linear regression), logistic regression predicts the log-odds of the outcome.\n",
        "\n",
        "Log-odds (logit) = log(P / (1-P))\n",
        "\n",
        "The model is linear in the log-odds space:\n",
        "\n",
        "\n",
        "3. How It Differs from Linear Regression\n",
        "Aspect\tLinear Regression\tLogistic Regression\n",
        "Task\tPredict continuous numeric values\tPredict probability / class labels\n",
        "Output Range\tAny real number (-‚àû, ‚àû)\tProbability [0,1]\n",
        "\n",
        "Loss Function\tMean Squared Error (MSE)\tLog-Loss / Cross-Entropy\n",
        "Assumption\tLinear relationship between X and y\tLinear relationship between X and log-odds\n",
        "Use Case Example\tPredict house price, temperature\tPredict loan default, disease (yes/no)\n",
        "4. Intuition\n",
        "\n",
        "Linear regression: Fits a straight line to predict numbers.\n",
        "\n",
        "Logistic regression: Fits an ‚ÄúS-shaped‚Äù curve (sigmoid) to output probabilities, which are then mapped to classes."
      ],
      "metadata": {
        "id": "i385QLetIc_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "Answer:\n",
        "1. What is the Sigmoid Function?\n",
        "\n",
        "The sigmoid function (also called logistic function) is defined as:\n",
        "\t‚Äã\n",
        "\n",
        "Properties of sigmoid:\n",
        "\n",
        "Outputs values in the range 0 to 1 ‚Üí perfect for probabilities.\n",
        "\n",
        "S-shaped curve (hence ‚Äúlogistic‚Äù).\n",
        "\n",
        "Smooth and differentiable ‚Üí important for gradient-based optimization.\n",
        "\n",
        "2. Role of Sigmoid in Logistic Regression\n",
        "\n",
        "Convert linear output to probability:\n",
        "\n",
        "Linear combination\n",
        "ùëß\n",
        "z can be any real number (-‚àû, ‚àû).\n",
        "\n",
        "Sigmoid squashes it to a value between 0 and 1:\n",
        "\n",
        "\n",
        "This represents the probability that the output belongs to class 1.\n",
        "\n",
        "Enable classification:\n",
        "\n",
        "Once we have a probability, we can set a threshold (commonly 0.5) to assign a class:\n",
        "\n",
        "\n",
        "Support optimization with gradient descent:\n",
        "\n",
        "The sigmoid function is differentiable, so we can compute gradients of the loss function (log-loss / cross-entropy) to update weights efficiently.\n",
        "\n",
        "3. Intuition\n",
        "\n",
        "Sigmoid turns this score into a probability, mapping negative scores to near 0 and positive scores to near 1.\n",
        "\n",
        "This allows logistic regression to make probabilistic predictions, rather than just numeric outputs like linear regression.\n",
        "\n",
        "Visual Example"
      ],
      "metadata": {
        "id": "pArSV0pbI2G1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Answer:\n",
        "1. What is Regularization?\n",
        "\n",
        "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the model‚Äôs loss function.\n",
        "\n",
        "In Logistic Regression, the standard loss function is the log-loss / cross-entropy:\n",
        "Regularization adds a penalty on the size of the coefficients (\n",
        "ùõΩ\n",
        "Œ≤):\n",
        "\n",
        "L2 Regularization (Ridge):\n",
        "Penalizes large weights by squaring them.\n",
        "\n",
        "Helps keep the model coefficients small and stable.\n",
        "\n",
        "L1 Regularization (Lasso):\n",
        "\n",
        "Encourages sparsity, meaning some coefficients can become exactly zero.\n",
        "\n",
        "Useful for feature selection.\n",
        "\n",
        "Elastic Net: Combination of L1 + L2 penalties.\n",
        "\n",
        "2. Why Regularization is Needed in Logistic Regression\n",
        "\n",
        "Prevent Overfitting:\n",
        "\n",
        "Without regularization, the model might assign very large weights to some features to perfectly fit the training data.\n",
        "\n",
        "Large weights ‚Üí poor generalization on unseen data.\n",
        "\n",
        "Handle Multicollinearity:\n",
        "\n",
        "If features are highly correlated, coefficients can become unstable.\n",
        "\n",
        "Regularization reduces variance and stabilizes the solution.\n",
        "\n",
        "Feature Selection (L1):\n",
        "\n",
        "Automatically removes irrelevant features by setting their coefficients to zero.\n",
        "\n",
        "Better Generalization:\n",
        "\n",
        "Produces a simpler, more robust model that works well on test/unseen data.\n",
        "\n",
        "3. Intuition\n",
        "\n",
        "Think of logistic regression trying to ‚Äústretch‚Äù a decision boundary to fit data.\n",
        "\n",
        "Without regularization ‚Üí it might overstretch, creating a complex boundary.\n",
        "\n",
        "With regularization ‚Üí it restrains the coefficients, leading to a smoother, more generalizable decision boundary.\n",
        "\n",
        "4. Example in scikit-learn"
      ],
      "metadata": {
        "id": "9xxOQJpEJlMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', C=1.0)  # C = 1/Œª\n",
        "\n",
        "# L1 regularization\n",
        "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n"
      ],
      "metadata": {
        "id": "BsdLlXXsJdor"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "Answer:\n",
        "1. Common Evaluation Metrics for Classification\n",
        "a) Accuracy\n",
        "\n",
        "What it measures: Overall correctness of the model.\n",
        "\n",
        "Use case: Works well when classes are balanced.\n",
        "\n",
        "Limitation: Misleading for imbalanced datasets.\n",
        "\n",
        "b) Precision\n",
        "Precision\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "What it measures: Of all the positive predictions, how many were actually positive.\n",
        "\n",
        "Importance: High precision ‚Üí few false positives.\n",
        "\n",
        "Use case: Fraud detection, spam detection (where false positives are costly).\n",
        "\n",
        "c) Recall (Sensitivity / True Positive Rate)\n",
        "Recall\n",
        "\n",
        "\n",
        "\n",
        "What it measures: Of all actual positives, how many did the model correctly identify.\n",
        "\n",
        "Importance: High recall ‚Üí few false negatives.\n",
        "\n",
        "Use case: Medical diagnosis, loan default prediction (missing a positive can be costly).\n",
        "\t‚Äã\n",
        "\n",
        "What it measures: Harmonic mean of precision and recall.\n",
        "\n",
        "Importance: Balances false positives and false negatives.\n",
        "\n",
        "Use case: Imbalanced datasets where both false positives and false negatives matter.\n",
        "\n",
        "e) Confusion Matrix\n",
        "\tPredicted Positive\tPredicted Negative\n",
        "Actual Positive\tTrue Positive (TP)\tFalse Negative (FN)\n",
        "Actual Negative\tFalse Positive (FP)\tTrue Negative (TN)\n",
        "\n",
        "What it measures: Gives a detailed breakdown of predictions.\n",
        "\n",
        "Importance: Helps understand model behavior beyond a single metric.\n",
        "\n",
        "f) ROC-AUC (Receiver Operating Characteristic ‚Äì Area Under Curve)\n",
        "\n",
        "ROC Curve: Plots True Positive Rate (Recall) vs False Positive Rate at different thresholds.\n",
        "\n",
        "AUC: Measures overall separability between classes (0.5 = random, 1.0 = perfect).\n",
        "\n",
        "Use case: Imbalanced datasets, probability-based models.\n",
        "\n",
        "g) PR-AUC (Precision-Recall AUC)\n",
        "\n",
        "Especially useful for highly imbalanced datasets.\n",
        "\n",
        "Measures the trade-off between precision and recall across thresholds.\n",
        "\n",
        "2. Why These Metrics Are Important\n",
        "\n",
        "Different metrics highlight different errors:\n",
        "\n",
        "Accuracy alone can be misleading if one class dominates.\n",
        "\n",
        "Precision and recall focus on false positives vs false negatives.\n",
        "\n",
        "Decision-making context matters:\n",
        "\n",
        "Banking ‚Üí minimize false negatives (loan defaults missed) ‚Üí prioritize recall.\n",
        "\n",
        "Spam detection ‚Üí minimize false positives ‚Üí prioritize precision.\n",
        "\n",
        "Helps tune and compare models:\n",
        "\n",
        "Metrics allow you to select the best model for your business goal.\n",
        "\n",
        "‚úÖ In short:\n",
        "\n",
        "Accuracy: Overall correctness\n",
        "\n",
        "Precision: Correct positive predictions\n",
        "\n",
        "Recall: Captured actual positives\n",
        "\n",
        "F1-score: Balance of precision & recall\n",
        "\n",
        "Confusion Matrix: Detailed breakdown\n",
        "\n",
        "ROC-AUC / PR-AUC: Performance across thresholds\n"
      ],
      "metadata": {
        "id": "_TWrM-O7KBtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "n6AeMe-BKgJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split data into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)  # Increased max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUQ8PhVjJ9XE",
        "outputId": "962c64ac-0770-4a88-cb7b-39e420785649"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Test Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:"
      ],
      "metadata": {
        "id": "K4sPFG_Giwqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with L2 regularization (Ridge)\n",
        "# L2 is the default regularization in LogisticRegression\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', multi_class='auto', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print model coefficients and accuracy\n",
        "print(\"Model Coefficients:\")\n",
        "for i, class_label in enumerate(model.classes_):\n",
        "    print(f\"Class {class_label}: {model.coef_[i]}\")\n",
        "\n",
        "print(f\"\\nIntercepts: {model.intercept_}\")\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "4jdzIZi5K5L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd09052-5f9f-49bd-f036-e560674de92e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "Class 0: [-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            "Class 1: [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            "Class 2: [-0.11497673 -0.70769055  2.58813565  1.7744936 ]\n",
            "\n",
            "Intercepts: [  9.00884295   1.86902164 -10.87786459]\n",
            "\n",
            "Test Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "CZAH3tEtjBeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObDeSTRDi-Rb",
        "outputId": "a222047b-eab9-4526-c725-58da0ebbe7fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "wOEnYL-pjSrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import warnings\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid (only penalties supported by solver)\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']  # Both work with liblinear\n",
        "}\n",
        "\n",
        "# Logistic Regression model\n",
        "log_reg = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=500)\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and validation accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "\n",
        "# Evaluate on test data\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnIz6tkBjPZZ",
        "outputId": "a59b6960-8577-4340-f0d3-cde98066165f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.9583333333333334\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "5YVsItOsjsKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Logistic Regression WITHOUT scaling\n",
        "model_no_scale = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "# 2. Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression WITH scaling\n",
        "model_scale = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model_scale.fit(X_train_scaled, y_train)\n",
        "y_pred_scale = model_scale.predict(X_test_scaled)\n",
        "accuracy_scale = accuracy_score(y_test, y_pred_scale)\n",
        "\n",
        "# Compare results\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_scale:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8vOpbt2jaDb",
        "outputId": "849a07d3-d4ef-4457-ae07-fb33e91611a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.0000\n",
            "Accuracy with scaling:    0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you‚Äôd take to build a Logistic Regression model ‚Äî including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world businessuse case.\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. Problem Understanding\n",
        "\n",
        "Goal: Predict which customers are likely to respond to a marketing campaign.\n",
        "\n",
        "Business Value: Focus marketing budget on customers with a high predicted probability of response ‚Üí higher ROI.\n",
        "\n",
        "Challenge: Only 5% positive class (severe imbalance).\n",
        "\n",
        "2. Data Preprocessing\n",
        "üîπ Data Cleaning\n",
        "\n",
        "Handle missing values:\n",
        "\n",
        "Numerical: Fill with median or use KNN imputer.\n",
        "\n",
        "Categorical: Fill with mode or \"Unknown\".\n",
        "\n",
        "Remove duplicates, outliers if they‚Äôre data errors.\n",
        "\n",
        "üîπ Feature Engineering\n",
        "\n",
        "Create features like:\n",
        "\n",
        "Customer purchase history (frequency, recency, monetary value).\n",
        "\n",
        "Demographics (age, location).\n",
        "\n",
        "Engagement metrics (email clicks, website visits).\n",
        "\n",
        "Convert categorical variables using One-Hot Encoding or Target Encoding.\n",
        "\n",
        "3. Train-Test Split\n",
        "\n",
        "Use stratified split to maintain the 5% response ratio in train and test sets.\n"
      ],
      "metadata": {
        "id": "FwK2DfqRj7P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "KSeS0lSWj4z4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Feature Scaling\n",
        "\n",
        "Logistic Regression is sensitive to feature magnitude ‚Üí StandardScaler or MinMaxScaler is essential."
      ],
      "metadata": {
        "id": "8H1bQ7wykYwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "-RhOPE5bkTEL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Handling Imbalanced Data\n",
        "\n",
        "Option 1: Class Weights (Preferred for LR)"
      ],
      "metadata": {
        "id": "koOjft0wkeNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(class_weight='balanced')\n"
      ],
      "metadata": {
        "id": "fsRsNevwka7R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This automatically adjusts weights inversely proportional to class frequency.\n",
        "\n",
        "Option 2: Oversampling / SMOTE"
      ],
      "metadata": {
        "id": "qrfWWzCnkjEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "yJ4qbsOMkgfZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 3: Undersampling for large datasets.\n",
        "\n",
        "6. Hyperparameter Tuning\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV to tune:\n",
        "\n",
        "C: Regularization strength.\n",
        "\n",
        "penalty: 'l1' (sparse) or 'l2' (default).\n",
        "\n",
        "class_weight: 'balanced' vs None."
      ],
      "metadata": {
        "id": "MagdmWKjkofD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C':[0.01,0.1,1,10],'penalty':['l1','l2']}\n"
      ],
      "metadata": {
        "id": "8fzkAiDHklfH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Model Evaluation\n",
        "\n",
        "Accuracy is misleading (95% accuracy if you predict ‚Äúno‚Äù for everyone).\n",
        "Instead, use:\n",
        "\n",
        "Precision, Recall, F1-Score (focus on Recall if catching responders is key).\n",
        "\n",
        "ROC-AUC and PR-AUC (Precision-Recall curve is more informative for imbalanced data).\n",
        "\n",
        "Confusion Matrix for business insight.\n",
        "\n",
        "8. Business Interpretation\n",
        "\n",
        "Use predicted probabilities ‚Üí rank customers by response likelihood.\n",
        "\n",
        "Send campaigns only to top X% predicted customers to save cost.\n",
        "\n",
        "Monitor model performance over time (data drift)."
      ],
      "metadata": {
        "id": "t1lWTVmWku1h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNc5tqk_ksHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}